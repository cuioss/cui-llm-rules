= Java Test Maintenance Process
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

[IMPORTANT]
====
This document is governed by the general process rules defined in xref:general.adoc[General Process Rules].
====

== Purpose

This document defines the comprehensive process for maintaining and improving Java test quality while preserving functionality and adhering to CUI standards. It provides systematic approaches for identifying, prioritizing, and resolving test quality issues across Java codebases.

== Related Documentation

* xref:../testing/core-standards.adoc[Core Testing Standards] - Fundamental testing requirements and patterns
* xref:../testing/quality-standards.adoc[Testing Quality Standards] - Quality requirements and AI artifact detection
* xref:../testing/cui-test-generator-guide.adoc[CUI Test Generator Guide] - Implementation guide for CUI testing framework
* xref:task-completion-standards.adoc[Task Completion Standards] - Quality verification processes
* xref:git-commit-standards.adoc[Git Commit Standards] - Standardized commit messaging
* xref:general.adoc[General Process Rules] - Overarching process governance

== Pre-Maintenance Checklist

**Requirements**: Execute the following verification steps before starting test maintenance, following xref:task-completion-standards.adoc[Task Completion Standards]:

1. [ ] **Build Verification**: `./mvnw -Ppre-commit clean verify -DskipTests`
2. [ ] **Test Execution**: `./mvnw clean test` - all tests must pass
3. [ ] **Coverage Baseline**: `./mvnw clean verify -Pcoverage` - record current metrics
4. [ ] **Module Identification**: List all modules for systematic processing

== Module-by-Module Approach

**Strategy**: Process modules systematically to maintain build stability and ensure comprehensive coverage.

=== Single Module Process

**Requirements**: Complete each module entirely before proceeding to the next:

1. **Module Focus**: Process one module completely before moving to next
2. **Test Execution**: `./mvnw clean test -pl module-name`
3. **Coverage Check**: `./mvnw clean verify -Pcoverage -pl module-name`
4. **Commit per Module**: Complete module before next following xref:git-commit-standards.adoc[Git Commit Standards]

=== Multi-Module Strategy

**Dependencies Management**:

* Process modules in dependency order (dependencies first)
* Maintain build stability after each module
* Verify inter-module test compatibility

== Test Quality Issues

**Overview**: This section defines the identification and resolution processes for common test quality issues found in Java codebases.

=== AI-Generated Code Detection

**Purpose**: Identify and systematically remove AI-generated code artifacts that compromise test quality.

**Process Requirements**:

* **Detection Process**: Identify AI artifacts per xref:../testing/quality-standards.adoc#ai-generated-code-detection[Quality Standards AI Detection]
* **Removal Actions**: Apply removal rules as defined in quality standards
* **Reflection Workarounds**: Ensure that there are no workaround tests using reflection to change internal state - this is always a bug
* **Process Focus**: Systematic identification and elimination during maintenance cycles

=== Test Enhancement Prioritization

**High Priority - Business Logic Tests:**

* **Unit Test Coverage**: Each type MUST have a separate dedicated unit test that focuses exclusively on that class, including comprehensive corner cases and edge cases. Always verify that aspects testable within that unit are not duplicated in other unit tests
* Domain object tests (token content, claims, validators)
* Business rule validation tests
* API contract and behavior tests
* User-facing functionality tests
* Security and compliance tests

**Medium Priority - Value Objects:**

* Data transfer objects with equals/hashCode contracts
* Configuration objects used in business logic
* Domain enums with complex behavior

**Low Priority - Infrastructure Tests:**

* HTTP client/server communication tests
* JWKS loading and caching tests
* Security infrastructure tests (if already comprehensive)
* Build and deployment infrastructure tests
* Framework integration tests

**Infrastructure Test Criteria:**

Tests that should remain as infrastructure (no CUI enhancement needed):

* Test external service integrations
* Test framework behavior rather than business logic
* Already provide adequate coverage through existing patterns
* Would not benefit from generator-based test data

**Classification Documentation Requirements:**

* Maintain test file inventory with enhancement status
* Document justification for infrastructure classification
* Track enhancement completion and coverage impact

=== Duplicate Detection

* **Identical Test Logic**: Consolidate into parameterized tests using JUnit 5 with @GeneratorsSource
* **Similar Test Data**: Extract to shared TypeGenerator implementations
* **Repeated Setup**: Move to `@BeforeEach` or test base classes
* **Copy-Paste Patterns**: Refactor into reusable test methods with generator support
* **Manual Data Duplication**: Replace with consistent generator-based data creation

== CUI Framework Compliance

**Overview**: Ensure all test code adheres to CUI framework requirements and follows established testing patterns.

=== Framework Compliance Requirements

**Standards Application**:

* **Apply Standards**: Follow xref:../testing/core-standards.adoc#cui-framework-requirements[CUI Framework Requirements]
* **Library Restrictions**: Adhere to xref:../testing/quality-standards.adoc#testing-library-restrictions[Testing Library Requirements]
* **Implementation Guide**: Reference xref:../testing/cui-test-generator-guide.adoc[CUI Test Generator Guide] for examples


=== Framework Migration Process

1. **Scan for Issues**: Identify violations of CUI framework requirements
2. **Apply Standards**: Follow migration patterns per framework standards
3. **Verify Compliance**: Check against CUI framework requirements
4. **Update Implementation**: Use generator guide examples for correct patterns

== Maintenance Standards

**Overview**: Define quality requirements for test structure and coverage that must be maintained throughout the maintenance process.

=== Test Structure

**Requirements**: Following xref:../testing/core-standards.adoc[Core Standards]:

* Verify AAA pattern (Arrange-Act-Assert)
* Ensure test independence
* Confirm descriptive test names
* Check proper `@DisplayName` usage

==== Assert Statement Messages

**Mandatory Requirements**:

* **Always Present**: Every assert statement MUST have a descriptive message
* **Preserve Existing**: Keep existing messages during maintenance - do not remove them
* **Add Missing**: Add descriptive messages to any assert statements lacking them
* **Review Quality**: During maintenance, verify that existing messages are:
  - Still correct and accurate for the test condition
  - Meaningful and helpful for debugging failures
  - Clear about what was expected vs what occurred

**Message Guidelines**:

* Describe the business condition being tested, not just the technical assertion
* Include relevant context (e.g., "User with admin role should have access to all resources")
* Make failure diagnosis easier by explaining why the assertion matters

=== Coverage Requirements

**Quality Gates**:

* Maintain minimum 80% line coverage
* Preserve existing coverage levels
* Identify untested critical paths
* Document coverage gaps

== Critical Constraints

**Overview**: Mandatory constraints that govern all test maintenance activities to ensure code safety and process integrity.

=== Production Code Protection

**Strict Requirements**:

* **NO PRODUCTION CHANGES** except confirmed bugs
* **Bug Discovery**: Must ask user for approval before fixing production code
* **Test-Only Changes**: Focus solely on test improvement
* **Behavior Preservation**: All existing tests must continue to pass

=== Bug Handling Process

**When production bugs are discovered**:

1. **Stop maintenance process**
2. **Document bug details** (location, issue, impact)
3. **Ask user for approval** to fix production code
4. **Wait for confirmation** before proceeding
5. **Create separate commit** for bug fix following xref:git-commit-standards.adoc[Git Commit Standards]

== Maintenance Workflow

=== Analysis Phase

1. **Scan for Issues**: Identify AI artifacts, duplicates, and non-compliance
2. **Unit Test Coverage Audit**: Verify each type has dedicated unit test focusing exclusively on that class with comprehensive corner/edge cases. Identify test duplication across unit tests
3. **AI Pattern Detection**: Check for method names >75 chars, excessive comments, verbose @DisplayName
4. **Non-Sensible Test Review**: Identify meaningless constructor tests and framework behavior tests
5. **CUI Framework Audit**: Check for manual data creation and missing annotations
6. **Value Object Review**: Identify objects needing contract testing per value object criteria
7. **Security Test Review**: Verify security testing patterns and vulnerability coverage
8. **Classify Test Files**: Apply prioritization framework (High/Medium/Low priority)
9. **Document Classification**: Maintain test file inventory with enhancement status and justification
10. **Prioritize Changes**: Focus on high-priority business logic tests first
11. **Plan Module Order**: Dependencies first, then dependent modules

=== Compliance Verification Checklist

For each test class, verify compliance with:

- [ ] **Unit Test Focus**: Each type has dedicated unit test focusing exclusively on that class with comprehensive corner/edge case coverage
- [ ] **Test Isolation**: Verify no duplication of unit-testable aspects across different unit tests
- [ ] **CUI Framework Standards**: xref:../testing/core-standards.adoc#cui-framework-requirements[Framework Requirements]
- [ ] **Quality Standards**: xref:../testing/quality-standards.adoc#ai-generated-code-detection[AI Detection] and xref:../testing/quality-standards.adoc#testing-library-restrictions[Library Restrictions]
- [ ] **Coverage Requirements**: xref:../testing/quality-standards.adoc#coverage-requirements[Coverage Standards]

=== Implementation Phase

1. **Apply Changes**: Fix one category of issues at a time
2. **Verify Tests**: `./mvnw clean test -pl module-name` after each change
3. **Check Coverage**: Ensure no coverage regression
4. **Commit Incrementally**: Small, focused commits per improvement type

=== Verification Phase
Following xref:task-completion-standards.adoc[Task Completion Standards]:

**Build Profile Standards:**

1. **Quality Build (Fast Feedback)**: `./mvnw -Ppre-commit clean verify -DskipTests -pl <module>`

   * Purpose: Code quality checks without test execution
   * Usage: Development iteration, pre-commit validation
   * Includes: License headers, compilation, static analysis

2. **Verification Build (Comprehensive)**: `./mvnw clean verify -pl <module>`

   * Purpose: Full test suite execution with comprehensive validation
   * Usage: Release preparation, CI/CD pipelines
   * Includes: Complete test suite, coverage analysis, quality gates

3. **Coverage Analysis**: `./mvnw clean verify -Pcoverage -pl <module>`

   * Purpose: Detailed coverage analysis and reporting
   * Usage: Coverage verification, regression detection
   * Includes: JaCoCo reports, threshold enforcement

**Verification Steps:**

1. **Quality Build**: Execute quality build for rapid feedback
2. **Complete Test Suite**: Execute verification build for comprehensive validation
3. **Coverage Verification**: Execute coverage build for detailed metrics
4. **Final Commit**: Consolidate if needed, update module status

**Important Notes:**

* No `-Pverification` profile exists - use standard `verify` goal
* Use module-specific execution with `-pl <module>` for faster builds
* Combine profiles as needed: `-Ppre-commit,coverage`

== Common Improvements

=== Test Simplification Process

* **Ensure Unit Test Focus**: Verify each type has dedicated unit test with comprehensive corner/edge case coverage and eliminate test duplication across units
* **Apply AI artifact removal** per xref:../testing/quality-standards.adoc#ai-generated-code-detection[Quality Standards]
* **Remove non-sensible tests** per quality standards criteria
* **Refactor complex test logic** to follow AAA pattern per xref:../testing/core-standards.adoc[Core Standards]
* **Avoid Excessive AAA Comments**: Do not add comments like `// Arrange`, `// Act`, `// Assert` unless the test structure is genuinely unclear - the code should be self-documenting through proper structure
* **Extract repeated test data** to TypeGenerator implementations
* **Convert similar tests** to parameterized tests using @GeneratorsSource
* **Fix compliance violations** per xref:../testing/quality-standards.adoc#sonarqube-compliance[SonarQube Standards]

=== Framework Migration Process

**Migration Steps:**

1. **Identify Violations**: Scan for manual data creation, hardcoded values, non-CUI frameworks
2. **Apply Standards**: Follow patterns per xref:../testing/cui-test-generator-guide.adoc[CUI Test Generator Guide]
3. **Verify Compliance**: Check against xref:../testing/quality-standards.adoc#cui-testing-utilities[CUI Testing Standards]
4. **Test Execution**: Ensure all tests pass after migration

=== Value Object Testing Process

**Apply ShouldHandleObjectContracts<T> when:**

* Class implements custom equals()/hashCode() methods
* Class represents domain data with value semantics
* Class is used in collections or as map keys
* Class participates in caching or persistence operations

**Do NOT apply to:**

* Enums (already have proper equals/hashCode from Java)
* Utility classes with only static methods
* Infrastructure classes (parsers, validators, builders)
* Classes that don't represent business value objects
* Builder pattern classes (test the built object instead)

**Implementation Steps:**

1. **Identify Value Objects**: Locate classes requiring contract testing using above criteria
2. **Apply Standards**: Follow patterns per xref:../testing/cui-test-generator-guide.adoc[CUI Test Generator Guide]
3. **Verify Coverage**: Ensure equals(), hashCode(), toString(), and Serializable contracts are tested
4. **Generator Integration**: Use cui-test-generator for all test data creation

**Common Mistakes to Avoid:**

* Applying contracts to enums (unnecessary)
* Testing infrastructure classes as value objects
* Mixing business logic tests with contract-only test classes

=== Structure Optimization

* Group related tests in inner classes
* Extract common setup to base classes  
* Simplify test resource management
* Improve test readability

For complete quality verification, see xref:task-completion-standards.adoc[Task Completion Standards].

== See Also

**Core Documentation**:

* xref:../testing/core-standards.adoc[Core Testing Standards] - Fundamental testing requirements
* xref:../testing/quality-standards.adoc[Testing Quality Standards] - Quality requirements and compliance
* xref:../testing/cui-test-generator-guide.adoc[CUI Test Generator Guide] - Implementation guide

**Process Documentation**:

* xref:task-completion-standards.adoc[Task Completion Standards] - Quality verification processes
* xref:git-commit-standards.adoc[Git Commit Standards] - Commit message standards
* xref:general.adoc[General Process Rules] - Overarching process governance
