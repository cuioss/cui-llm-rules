= Quality Standards and Testing Framework
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

== Purpose

This document defines comprehensive quality standards, testing framework guidelines, and quality verification processes for CUI OSS projects.

== Related Documentation

* xref:core-standards.adoc[Testing Standards Overview]
* xref:core-standards.adoc[Core Testing Standards]
* xref:cui-test-generator-guide.adoc[CUI Test Generator Usage Guide]
* xref:../logging/testing-guide.adoc[Logging Testing Guide]
* xref:../documentation/javadoc-standards.adoc[Javadoc Standards]
* xref:../java/java-code-standards.adoc[Java Standards]

== Core Testing Standards

=== Test Structure and Organization

* Follow AAA pattern (Arrange-Act-Assert)
* One logical assertion per test
* Clear test naming convention
* Descriptive test documentation
* Independent test execution
* Clean test environment
* Predictable test data

[[coverage-requirements]]
=== Coverage Requirements

* Minimum 80% line coverage
* Minimum 80% branch coverage
* Critical paths must have 100% coverage
* All public APIs must be tested
* Edge cases must be covered
* No coverage regressions allowed
* Regular coverage review
* Use Maven profile `-Pcoverage` for verifying coverage metrics

=== Testing Tools and Frameworks

[[testing-library-restrictions]]
==== Testing Library Requirements

Strict compliance with approved testing libraries to ensure consistency and compatibility:

**ALLOWED LIBRARIES:**

* **cui-test-libs**: All CUI testing utilities and frameworks (required for CUI projects)
* **junit-jupiter**: JUnit 5 for all test execution and assertions
* **awaitility**: For asynchronous testing, waiting conditions, and polling
* **rest-assured**: For REST API testing, HTTP request/response validation

**FORBIDDEN LIBRARIES:**

* **Mockito**: Do NOT use for mocking - use CUI framework alternatives instead
* **PowerMock**: Do NOT use for advanced mocking scenarios - refactor to use dependency injection
* **Hamcrest**: Do NOT use for assertions - use JUnit 5 assertions exclusively

**Migration Guidelines:**

* Replace Mockito with CUI framework dependency injection patterns
* Convert Hamcrest matchers to JUnit 5 assertion methods
* Use cui-test-mockwebserver-junit5 for HTTP mocking instead of Mockito
* Leverage cui-test-generator for test data creation instead of manual or random approaches

==== Required Frameworks

1. JUnit 5
   * Use `@DisplayName` for readable test names
   * Leverage parameterized tests (see <<parameterized-tests-best-practices,Parameterized Tests Best Practices>>)
   * Apply proper test lifecycle annotations

==== CUI Testing Utilities

1. **CUI Framework Compliance**: All testing must follow xref:core-standards.adoc#cui-framework-requirements[CUI Framework Requirements]
   * See xref:cui-test-generator-guide.adoc[CUI Test Generator Usage Guide] for implementation examples
   * See xref:core-standards.adoc[Core Standards] for mandatory framework requirements

2. https://gitingest.com/github.com/cuioss/cui-jsf-test-basic[cui-jsf-test-basic]
   * Use for JSF component testing
   * Follow component test patterns
   * Include lifecycle tests

4. https://gitingest.com/github.com/cuioss/cui-test-mockwebserver-junit5[cui-test-mockwebserver-junit5]
   * Use for testing HTTP client interactions
   * Mock HTTP server responses
   * Test request/response handling

[[parameterized-tests-best-practices]]
==== Parameterized Tests Best Practices

Parameterized tests are **mandatory** when testing at least 3 similar variants of the same behavior. This ensures comprehensive test coverage while maintaining clean, maintainable test code.

===== When to Use Parameterized Tests

* **Mandatory Usage**: When you have 3 or more similar test cases that differ only in input data or expected outcomes
* **Code Consolidation**: Replace multiple identical test methods with a single parameterized test
* **Data-Driven Testing**: Test the same logic with different input combinations

===== Annotation Hierarchy (Preferred Order)

1. **@GeneratorsSource** (Most Preferred)
   * Leverage CUI test generators for complex object creation
   * Provides comprehensive test data coverage
   * Maintains consistency with CUI framework standards
   * Example: Testing with various token configurations, user objects, or configuration beans

2. **@CompositeTypeGeneratorSource** (Highly Preferred)
   * Use for testing with multiple related complex types
   * Combines multiple generators for comprehensive scenarios
   * Ideal for integration-style unit tests

3. **@CsvSource** (Standard Choice)
   * Use for simple data combinations
   * Good for testing multiple input/output pairs
   * Easy to read and maintain
   * Example: `@CsvSource({"input1,expected1", "input2,expected2", "input3,expected3"})`

4. **@ValueSource** (Simple Cases)
   * Use when testing with single parameter variations
   * Suitable for boundary value testing
   * Example: `@ValueSource(strings = {"", "   ", "null"})`

5. **@MethodSource** (Last Resort)
   * Only use when other options are insufficient
   * Required for complex data setup that cannot be handled by generators
   * Must provide clear justification in test documentation

===== Implementation Examples

.Generator-Based Parameterized Test (Preferred)
[source,java]
----
@ParameterizedTest
@DisplayName("Should validate tokens with different configurations")
@GeneratorsSource(TokenConfigGenerator.class)
void shouldValidateTokensWithDifferentConfigurations(TokenConfig config) {
    // Test implementation using generated configuration
    assertDoesNotThrow(() -> validator.validate(token, config));
}
----

.CSV-Based Parameterized Test (Standard)
[source,java]
----
@ParameterizedTest
@DisplayName("Should validate issuer with different URL patterns")
@CsvSource({
    "https://example.com, https://example.com/.well-known/openid-configuration",
    "https://example.com/auth/realms/master, https://example.com/auth/realms/master/.well-known/openid-configuration",
    "https://example.com:443, https://example.com/.well-known/openid-configuration"
})
void shouldValidateMatchingIssuerSuccessfully(String issuer, String wellKnownUrl) {
    URL wellKnown = URI.create(wellKnownUrl).toURL();
    assertDoesNotThrow(() -> parser.validateIssuer(issuer, wellKnown));
}
----

===== Quality Requirements

* **Test Method Names**: Use descriptive `@DisplayName` annotations
* **Parameter Names**: Choose meaningful parameter names that clearly indicate their purpose
* **Documentation**: Comment complex parameter combinations or business logic
* **Consolidation**: Always refactor duplicate test patterns into parameterized tests
* **Generator Priority**: Prefer CUI generators over manual data creation for consistency with framework standards

=== Test Categories

==== Unit Tests

* Test single units in isolation
* Mock all dependencies
* Fast execution
* High maintainability

==== Integration Tests

* Test component interactions
* Minimal mocking
* Cover critical paths
* Include error scenarios
* Regular maintenance required

==== System Tests

* End-to-end scenarios
* Real dependencies where possible
* Cover main user flows
* Include performance criteria

== Quality Verification

For comprehensive quality verification processes, see xref:../process/task-completion-standards.adoc[Task Completion Standards].

=== Quality Analysis Tools

* SonarCloud for static code analysis
* JUnit for unit testing
* Mutation testing for test quality
* Regular code reviews
* Continuous integration checks (see xref:../process/task-completion-standards.adoc[Task Completion Standards])

=== Quality Metrics

* Code coverage
* Code duplication
* Complexity metrics
* Issue density
* Technical debt ratio

=== Best Practices

==== Test Quality

* Regular test review
* Mutation testing  
* Test failure analysis
* DRY in test utilities
* Clear test documentation
* Consistent patterns

[[ai-generated-code-detection]]
==== AI-Generated Code Detection and Elimination

**Critical Indicators of AI-Generated Test Code:**
* Method names exceeding 75 characters
* Excessive inline comments explaining obvious operations
* Repetitive test patterns with only minor variations
* Verbose @DisplayName annotations (54+ characters)
* Over-documentation with redundant explanations
* Meaningless constructor tests verifying trivial functionality

**Test Categories to Eliminate:**
1. **Meaningless Tests**: Tests that verify trivial functionality without business value
2. **Framework Behavior Tests**: Tests that verify framework functionality rather than application logic  
3. **Duplicate Logic Tests**: Tests that duplicate existing test scenarios without added value
4. **Over-Complex Unit Tests**: Tests that are disproportionately complex for the functionality being tested

**Removal Requirements:**
* **ELIMINATE**: Tests that verify trivial functionality without business value
* **REMOVE**: Comments explaining obvious operations
* **SIMPLIFY**: Overly verbose method and test names
* **CONSOLIDATE**: Identical test patterns into parameterized tests with @GeneratorsSource  
* **REPLACE**: Verbose @DisplayName with focused descriptions under 50 characters
* **PRESERVE**: Meaningful assertion messages that provide debugging context (see <<assertion-message-standards,Assertion Message Standards>>)
* **REMOVE ONLY**: Meaningless assertion messages like "Should be true" or overly verbose descriptions (100+ characters)

**Examples of AI Artifacts to Remove:**
```java
// REMOVE: Meaningless constructor test
@Test
void shouldCreateWithValidParameters() {
    assertNotNull(new AccessTokenContent(validClaims));
}

// REMOVE: Framework behavior test
@Test  
void shouldLogInfoMessageWhenTokenValidatorIsInitialized() {
    // Testing framework logging behavior, not application logic
}

// REMOVE: Excessive inline comments
@Test
void shouldValidateToken() {
    // Create a token holder for testing purposes
    TestTokenHolder holder = new TestTokenHolder();
    // Set the token type to access token for validation
    holder.setTokenType(ACCESS_TOKEN);
    // Perform validation and check result
    assertTrue(validator.validate(holder.build()).isValid());
}
```

[[sonarqube-compliance]]
==== Assertion Message Standards

===== Meaningful Assertion Messages

**Requirement:** ALL assertions must include meaningful, concise failure messages that provide context for debugging when tests fail.

**Key Principles:**

* **Include WHY the assertion should pass**: Explain the expected behavior being verified
* **Keep messages concise but informative**: Aim for 20-60 characters
* **Provide debugging context**: Help identify what went wrong when the test fails
* **Use consistent language**: Follow established patterns within the test suite

**Examples:**

**Good Assertion Messages:**
```java
assertTrue(result.isPresent(), "User should be found by valid ID");
assertEquals(expected, actual, "Token should contain correct issuer");
assertFalse(errors.isEmpty(), "Validation should detect invalid input");
assertNotNull(response, "API should return non-null response");
```

**Poor Assertion Messages (DO NOT USE):**
```java
assertTrue(result.isPresent()); // Missing context
assertTrue(result.isPresent(), "Should be true"); // Meaningless
assertTrue(result.isPresent(), "This assertion verifies that the result optional contains a value as expected when..."); // Too verbose
```

**Exception Testing:**
```java
TokenValidationException exception = assertThrows(TokenValidationException.class, 
    () -> validator.validate(invalidToken),
    "Invalid token should trigger validation exception");
```

**Standard Message Patterns:**
* For presence checks: `"X should be present"`, `"X should not be null"`
* For equality: `"X should equal Y"`, `"X should match expected value"`
* For collections: `"Collection should contain X"`, `"List should have Y elements"`
* For exceptions: `"X should throw Y exception"`, `"Invalid X should trigger Y"`

==== SonarQube Compliance

===== assertThrows Best Practices

Follow SonarQube rule: "Refactor the code of the lambda to have only one invocation possibly throwing a runtime exception"

**Problem:** Lambda expressions in `assertThrows` should contain only one statement that can throw the expected exception.

**Before (Violates SonarQube Rule):**
```java
@Test
void shouldThrowExceptionOnInvalidInput() {
    assertThrows(IllegalArgumentException.class, () -> {
        String input = "invalid";
        service.validateInput(input);
        service.processInput(input); // Multiple throwing statements
    });
}
```

**After (Compliant):**
```java
@Test
void shouldThrowExceptionOnInvalidInput() {
    // given
    String input = "invalid";
    service.validateInput(input); // Setup outside lambda
    
    // when/then
    assertThrows(IllegalArgumentException.class, () -> 
        service.processInput(input) // Only one throwing statement
    );
}
```


**Key Principles:**
* Move setup code outside the lambda expression
* Keep only the single method call that should throw the exception
* Use helper methods for complex throwing operations
* Maintain clear test structure with given/when/then pattern

=== Performance

* Fast test execution
* Efficient resource usage
* Parallel test execution where possible
* Regular performance monitoring

=== Review Process

Regular Review Points:

* After major feature completion
* Before creating pull requests
* During code review process
* Post-merge verification

=== Documentation

* Record quality findings
* Document remediation steps
* Note technical debt decisions
* Update quality metrics
* Track coverage changes

== Success Criteria

=== Test Coverage

* All coverage requirements met
* Critical paths fully covered
* Test quality sufficient
* No coverage regressions

=== Quality Analysis

For comprehensive quality gate processes, see xref:../process/task-completion-standards.adoc[Task Completion Standards].

* All quality gates passed
* New issues addressed
* Impact assessed
* Clear remediation paths
* Documentation complete

=== Security

* No critical vulnerabilities
* Security hotspots reviewed
* Dependencies verified
* Security standards met

== Maven Coverage Profile

For standardized build verification processes, see xref:../process/task-completion-standards.adoc[Task Completion Standards].

To verify code coverage in your project, use the Maven profile `-Pcoverage`:

For code coverage verification, use the coverage profile following the xref:../process/task-completion-standards.adoc[Task Completion Standards]:

[source,bash]
----
./mvnw clean verify -Pcoverage
----

This profile will:

* Enable JaCoCo code coverage analysis
* Generate detailed coverage reports
* Enforce minimum coverage thresholds
* Fail the build if coverage requirements are not met

== See Also

* xref:../java/java-code-standards.adoc[Java Standards]
* xref:../documentation/javadoc-standards.adoc[Javadoc Standards]
* xref:../logging/README.adoc[Logging Standards]
